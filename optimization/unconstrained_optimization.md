---
layout: post
---

# 1.无约束优化问题

先给定义，问题表示为

$$\min_{x}f(x)\\
where\ x\in\mathbb{R}^n,f:\mathbb{R}^n\rightarrow \mathbb{R}$$

这里我们考虑f是可微的。在找这个问题的解之前先明确什么样的点是该问题的一个解，即什么是极小值点。

- 如果对所有的x都有 $f(\hat x)\leqslant f(x)$，则称 $\hat x$ 为全局极小值点。

- 如果在 $\hat x$ 的邻域 $\mathcal N$ 内有 $f(\hat x)\leqslant f(x),\forall x\in\mathcal N$，则称 $\hat x$ 为局部极小值点。

- 如果 $f(\hat x)<f(x),\forall x\in\mathcal N,x\neq \hat x$，则 $\hat x$ 称为严格局部极小值点。

全局极小值条件比较苛刻，我们先考虑局部较小值。看上去我们需要比较邻域内的所有函数值才能确定一个点是不是局部极小值点，实际上如果函数f是二阶连续可微的，我们可以通过函数梯度 $\nabla f$ 和黑塞矩阵 $\nabla^2f$ 来判定。

$\hat x$ 是局部极小值的条件：

1. 一阶必要条件：$\nabla f(\hat x)=0$
2. 二阶必要条件：$\nabla f(\hat x)=0, \nabla^2f(\hat x)$ 半正定
3. 二阶充分条件：$\nabla f(\hat x)=0, \nabla^2f(\hat x)$ 正定，则 $\hat x$ 是严格极小值点

特别的，如果f是凸函数，则局部极小值就是全局极小值。如果该函数还可微，则导数为0的点就是全局极小值点。

实际情况中凸函数是很少的，对于一般函数我们通过迭代的方法，每次找到一个函数值更小的点，逐步找到最小值，每步迭代的关键就是 <font color=#FF0000>如何找到当前点邻域内的极小值点</font> 。根据找邻域内极小值方法的不同，无约束优化方法可分为两类，线搜索和置信域。

$$\left\{
  \begin{matrix}
   线搜索\left\{\begin{matrix}最速梯度\\牛顿法\\拟牛顿法\\共轭梯度法\end{matrix} \right.\\
   置信域\left\{\begin{matrix}柯西点\\折线法\\二维搜索\end{matrix} \right.
  \end{matrix}
\right.$$

# 2.方法预览和概述

## 线搜索（line search）

线搜索方法在迭代第k步的目标是

$$\min_{\alpha >0}f(x_k+\alpha p_k)$$

其中需要确定的变量为方向 $p_k$ 和步长 $\alpha$。步长的确定方法我们在后面介绍，先说如何确定搜索方向。最显而易见的搜索方向是负梯度的方向，因为在该方向上函数值的下降速度最快，证明如下：

$$f(x_k+\alpha p)=f(x_k)+\alpha p^T\nabla f_k+\frac{1}{2}\alpha ^2p^T\nabla ^2f(x_k+tp)p,\ for\ some\ t\in(0,\alpha)\tag 1$$

上式为泰勒展开，我们发现函数 f 在某一个方向 p 上的增量等于

$$\left.\frac{\partial f(x_k+\alpha p)}{\partial \alpha} \right| {}_{\alpha=0} = p^T\nabla f_k$$

我们希望找到一个方向p，该方向上函数f下降速度最大，即：

$$\min_{p}p^T\nabla f_k,\ subject\ to\ \|p\|=1$$

因为我们限制了p的模为1，所以上式简化为 $\|\nabla f_k\|\cos\theta$，其中 $\theta$ 为p和梯度之间的夹角。所以当p和梯度反向时，该目标最小，即 $p=-\nabla f_k/\|\nabla f_k\|$ 为函数f下降最快的方向。

**最速梯度下降法** 选择的搜索方向就是负梯度方向，它的计算量很小，但是它是线性收敛的，在接近最优解的地方收敛会很慢。由泰勒分解我们还可以看出，其实只要我们选择的方向和负梯度的夹角小于90度，在步长足够小的情况下都能保证函数f的下降，因为
$$p^T\nabla f_k=\|p\| \|\nabla f_k\|\cos\theta<0, \ \theta\in(\frac{\pi}{2},\frac{3\pi}{2})$$

**牛顿法** 就是选择了一个接近负梯度的方向作为搜索方向，由此获得了更好的收敛速度。怎么做的呢，首先用二次函数近似目标函数：

$$f(x_k+p)\approx f(x_k)+p^T\nabla f_k+\frac{1}{2}p^T\nabla ^2f_kp \overset{def}{=} m_k(p)$$

假设黑塞矩阵正定，目标函数近似为二次函数，二次函数是凸函数，最小值点就是导数等于0的点，所以对上式求导求零点为

$$p_k=-\nabla ^2f_k^{-1}\nabla f_k$$

牛顿方向的可靠性取决于近似函数m和f的相似程度，由于二者相差p的三阶无穷小量，所以在p很小的时候可以认为m对f的近似足够精确。如果黑塞矩阵正定，则可以保证牛顿方向是下降的(在梯度不为0的情况下)：

$$p_k^T\nabla f_k=-p_k^T\nabla ^2f_kp_k<0,\ p_k\neq 0$$

牛顿法的步长一般选择为1，除非经过计算认为该步长上函数f没有下降足够多。

如果黑塞矩阵不正定，则牛顿法会陷入麻烦，因为黑塞矩阵的逆都不一定存在了。这时会有一些修正方法，使搜索方向保持二次收敛特性的同时保证下降性，后面我可能会补充这些方法。牛顿法的优势是二次收敛，劣势是黑塞矩阵求逆计算量大。

**拟牛顿方法** 在每步迭代时估计一个矩阵B代替黑塞矩阵，减小计算量同时获得超线性收敛速度。实际上拟牛顿法根据梯度的变化量来估计黑塞矩阵，这是合理的，我们可以通过对梯度做泰勒展开得到

$$\nabla f_{k+1}=\nabla f_k+\nabla ^2f_k(x_{k+1}-x_k)+o(||x_{k+1}-x_k||)$$

这里我不知道书里是不是用k步的黑塞矩阵近似了k+1步的黑塞矩阵，因为书里得到了下式

$$\nabla ^2f_{k+1}(x_{k+1}-x_k)\approx \nabla f_{k+1}-\nabla f_{k}$$

拟牛顿法最终用B作为H的估计，使其满足

$$B_{k+1}s_k=y_k\\where\ s_k=x_{k+1}-x_k,\ y_k=\nabla f_{k+1}-\nabla f_{k}$$

再由

$$p_k=-B_k^{-1}\nabla f_k$$

确定搜索方向，实现迭代优化。

B还需要满足几个条件：1.是对称矩阵；2. $B_{k+1}$ 和 $B_k$ 之间相差一个低秩矩阵（？）。拟牛顿法一般过程为：给定一个B的初值，然后在前一步B的基础上加一个低秩矩阵（该矩阵由 $s_k,y_k$ 确定）得到新的B。B的更新方法后面随缘介绍。

除了估计H，还有的拟牛顿法直接估计H的逆，这样就连求逆的过程都省了，也是后面随缘介绍。

**共轭梯度法** 获得搜索方向的方法如下

$$p_k=-\nabla f_k+\beta_kp_{k-1}$$

确保了 $p_{k-1}$ 和 $p_{k}$ 是一对共轭的方向（后面再说…），共轭梯度法最早是用来解线性方程组的，而线性方程组 $Ax=b$ 的解也正好是二次函数 $f(x)=\frac{1}{2}x^TAx+b^Tx$ 极小值的解，所以该方法也可以用来做最优化。共轭梯度法和最速梯度下降法有相似的计算复杂度，收敛速度比最速梯度法快，比牛顿法和拟牛顿法慢，不需要保存矩阵。

*所有上述的找搜索方向的方法（除了共轭梯度法）都可以用于置信域方法中。*

## 置信域法（trust-region method）

思路：在迭代第k+1步时，在 $x_k$ 附近用一个函数 $m_k$ 近似函数f，求 $m_k$ 的极小值点 $x_{k+1}$ 作为该次估计的f的极小值点。由于近似只在局部有效，所以我们只在 $x_k$ 的邻域内搜索极小值，也就是置信域内。如果求得的极小值点上f的函数值没有明显下降（m对f的近似不好）则缩小范围，反之可以放大范围。

用数学表达，置信域法将优化问题转化为

$$\min_p m_k(x_k+p),\ where\ x_k+p\ lies\ inside\ the\ trust\ region$$

一般来说p被约束在球中，${\|p\|}_2\leqslant\Delta$， $\Delta$ 称为置信域半径。有时也可以用椭球或方形置信域。一般m会选择二次函数的形式

$$m_k(x_k+p)=f_k+p^T\nabla f_k+\frac{1}{2}p^TB_kp$$

m近似于f的泰勒展开，m和f在x点处的梯度是一样的，B可以是黑塞矩阵或其近似估计。B取不同值时，我们可以得到上述几种线搜索法的形式。

当B取0矩阵时，问题变为：

$$\min_{p}f_k+p^T\nabla f_k,\ subject\ to\ {\|p\|}_2\leqslant\Delta_k$$

解为

$$p_k=-\frac{\Delta_k \nabla f_k}{\|\nabla f_k\|}$$

这其实就是最速梯度法的解。

当B是黑塞矩阵时，我们就得到了置信域牛顿法。此时我们不需要黑塞矩阵正定也能找到极小值，因为我们划定了搜索范围。

当B是黑塞矩阵的估计时，我们就得到了置信域拟牛顿法。

所以，线搜索（除了共轭梯度法）和置信域法有一些相同点：都是用 m 来拟合 f，然后寻找 m 的极小值点来近似 f 的极小值点。其中 m 可以选择 f 泰勒展开的前几项作为估计，如果选择二次函数的形式，那么 B 可以选择黑塞矩阵或者其估计。不同点在于：线搜索法是在一个方向上寻找极小值；置信域法是在一个邻域内寻找极小值。

## 尺度

在实际问题中我们优化的自变量向量x，它的各个分量的数值可能相差很大，比如好几个数量级，我们可以选择给x做一个线性变换，得到新变量z，它的各个分量尺度比较相似，这样对z做优化，然后再逆变换得到x。 有一些方法对变量的尺度差比较敏感，比如最速梯度法在此时会收敛更慢，而牛顿法则对尺度差不敏感。

## 收敛速度

定义一下各种收敛速度，后面会提到。

${x_k}$ 是 $R^n$ 上收敛至 $x^{\star}$ 的序列，如果对所有足够大的k，存在常数 $r\in(0,1)$ 满足下式，

$$\frac{\|x_{k+1}-x^{\star}\|}{\|x_k-x^{\star}\|}\leqslant r$$

则称序列是Q-线性（Q-linear）收敛的。其中Q代表除法（quotient）。

如果满足

$$\lim_{k\rightarrow\infty}\frac{\|x_{k+1}-x^{\star}\|}{\|x_k-x^{\star}\|}=0$$

则称为Q-超线性收敛(Q-superlinear)。

如果对所有足够大的k满足

$$\frac{\|x_{k+1}-x^{\star}\|}{\|x_k-x^{\star}\|^2}\leqslant M$$

则称为Q-二次收敛(Q-quadratic)，其中M是常数，不一定要小于1。

# 3.线搜索

这个标题感觉打了无数遍了，终于要来看看到底怎么搜索。线搜索的迭代过程为

$$x_{k+1}=x_k+\alpha_k p_k$$

方向p一般选择为函数值下降的方向，一般有如下形式

$$p_k=-B_k^{-1}\nabla f_k$$

B是对称非奇异矩阵。如果B取单位阵，得到梯度下降法，B取黑塞矩阵H得到牛顿法，B取黑塞矩阵的估计得到拟牛顿法。如果B正定，则可以保证该方向上函数值下降：

$$p_k^T\nabla f_k=-\nabla f_k^TB_k^{-1}\nabla f_k<0$$

## 3.1确定步长

我们想在搜索方向上找到最小值，但是又不能把该方向上所有的函数值都算一遍，所以要在计算复杂度和函数值下降之间做一个权衡取舍。大部分线搜索方法会选择几个候选的步长，在其中找到一个满足预设条件(condition)的步长。总的来说分两步：1.确定步长所在区间；2.在该区间内二分法或差值寻找合适的步长。

下面介绍几种搜索终止条件，即步长需要满足的条件。

### the Wolfe conditions
