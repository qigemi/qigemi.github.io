---
layout: post
---

# 0.序

简要介绍最优化的方法，参考书籍:电子工业出版社《最优化导论（第四版）》[1]，《Numerical Optimization》[2]。前者侧重于知识介绍，类似教材或科普，后者侧重实际应用，非常硬核，列举了一些不同场景下应用各种优化算法的技巧。我还是主要侧重于知识科普，力图理清楚各种优化算法的区别与联系，各自特点。有一点很抱歉的是，很多地方我直接给了结论，没有记录太多数学证明和推导，因为实在懒得打公式了T_T。

本篇先记录一些基础知识，然后是最优化系列博客的个人总结纲要。

# 1.基础知识

## 线性代数
可参考我之前的博文。

## 几何

### 超平面

$$ \boldsymbol{u}=[u_1,...,u_n]^T\\
\boldsymbol{x}=[x_1,...,x_n]^T$$

则所有满足 $\boldsymbol{u}^T\boldsymbol{x}=v$ 的点 $\boldsymbol{x}$ 的集合称为空间 $\mathbb{R}^n$ 的超平面。即

$$\{\boldsymbol{x}\in \mathbb{R}^n|\boldsymbol{u}^T\boldsymbol{x}=v\}$$

可以看出，u即超平面的法线方向，超平面内所有向量都垂直于u，v=0时超平面过原点，超平面随v的变化延法线方向平移。定义：

$$H_+=\{\boldsymbol{x}\in \mathbb{R}^n|\boldsymbol{u}^T\boldsymbol{x}\geqslant v\}$$
$$H_-=\{\boldsymbol{x}\in \mathbb{R}^n|\boldsymbol{u}^T\boldsymbol{x}\leqslant v\}$$

分别称为正半空间和负半空间。

### 线性簇

$$\{\boldsymbol{x}\in \mathbb{R}^n|Ax=b\},A\in \mathbb R^{m\times n}$$

即线性方程组的解，如果解空间维数小于n，则它应该是有限个超平面的交集。

### 凸集

两点u，v之间线段上的点可以表示为 $w=\alpha u+(1-\alpha)v,\alpha \in [0,1]$，这也称作u，v的凸组合。如果对于所有 $u,v\in \Theta$，uv之间的线段都位于 $\Theta$ 内，则称 $\Theta$ 为凸集。

凸集可以是：空集，一个点，一条直线或线段，子空间，超平面，半空间，$\mathbb{R}^n$。

### 邻域

如果一个集合既是闭集又是有界集，则称为紧集。

魏尔斯特拉斯定理：假设 $f:\Omega\rightarrow\mathbb R$ 是一个连续函数，其中 $\Omega \subset \mathbb{R}^n$ 是紧集。那么必定存在点 $x_0\in \Omega$，使得对于所有 $x\in \Omega$ 的都有 $f(x_0)\leqslant f(x)$。也就是说，$f$ 能够在 $\Omega$ 上取得极小值。

### 多面体

过凸集边界上的点的超平面将空间分为两个半空间，如果凸集完全位于其中一个半空间内，则称该超平面为该集合的支撑超平面。

如果一个集合可以表示为有限个半空间的交集，则称为多面体。因为凸集的交集还是凸集，所以多面体是凸集。

多面体的包是刚好可以包含多面体的线性簇。比如点的包是它本身，线段的包是其所在直线。

每个k维多面体都有维数为k-1，k-2，……，1,0的多个面。其中0维面叫顶点，1维面叫棱。

## 微积分

这里从另一个角度理解导数。微积分的基本理念是用仿射函数对函数进行近似。仿射函数定义为

$$\mathcal A(x)=\mathcal{L}(x)+y$$

其中 $\mathcal{L}$ 是线性函数，为了仿射函数在点x0附近能够近似目标函数，则应该有，

$$\mathcal A(x_0)=f(x_0)\\
\Rightarrow y=f(x_0)-\mathcal{L}(x_0)\\
\Rightarrow \mathcal{L}(x)+y=\mathcal{L}(x)-\mathcal{L}(x_0)+f(x_0)=\mathcal{L}(x-x_0)+f(x_0)$$

然后，要求 $\mathcal A(x)$ 接近 $f(x)$ 的速度比x接近x0的速度快，

$$\lim_{x\rightarrow x_0,x\in \Omega}{\frac{|f(x)-\mathcal A(x)|}{|x-x_0|}}=0$$

给定函数 $f:\Omega\rightarrow\mathbb{R}^m,\Omega\subset\mathbb{R}^n$，如果存在一个仿射函数能够在x0点附近近似函数f，那么就称函数f在点x0处是可微的。即存在线性函数使得

$$\lim_{x\rightarrow x_0,x\in \Omega}{\frac{|f(x) - \mathcal{L}(x-x_0) + f(x_0)|}{|x - x_0|}}=0$$

其中可由f和x0唯一确定，称为f在x0处的导数。

注意仿射变换不一定是线性变换，因为线性变换必须把0变换成0。但导数是线性函数，它估计了在x0点附近f随x的变化量。所以导数对应一个线性变换，输入是 $x-x_0\in \mathbb{R}^n$，输出是函数变化的估计量 $\hat {\Delta f}\in \mathbb{R}^m$。所以导数可以写成矩阵的形式，矩阵表示 $L\in\mathbb{R}^{m\times n}$。

特殊的，如果f是单值函数，则导数矩阵为1行n列，此时可定义梯度

$$\nabla f(x)=Df(x)^T$$

如果梯度可微，则称f是二次可微的，梯度的导数称作黑塞矩阵（Hessian）。如果导数连续，则称为连续可微。如果函数f是二次连续可微的，则黑塞矩阵是对称的。

这里注意f是单值函数时才有梯度，黑塞矩阵才是个矩阵。我之前不明确的时候对这些概念还有些模糊，还在考虑输出向量的函数的梯度是什么样的等等。

# 2.大纲

因为两本书侧重不同，结构也不同，为了理清我自己的结构，先做个总结。

首先是优化问题的定义和分类。什么是优化问题，我觉得大家应该都有个印象，一般是求目标函数的极小值。优化问题可以按照几个标准分类。

- 离散和连续  
离散优化指解存在于一个有限的集合中，比如整数规划问题，自变量只能取整数。但是这个有限的数量级可能非常大，以至于不能用穷举的方法找到最优解。连续优化指在一个无限的集合（一般是实向量集合）中寻找解。连续优化问题更简单，因为我们知道每个点附近目标函数的性质。我们主要讨论连续优化。而离散优化求解时，往往也需要求解连续优化子问题。

- 有约束和无约束  
有约束问题一般会把约束条件融合进目标函数转化成无约束优化问题求解。

- 全局和局部优化  
局部优化只在当前点附近寻找最优解，更简单。全局优化则不一定能找到真正的最优解。全局优化问题一般分解成一系列的局部优化问题来求解。

- 随机和确定问题  
如果优化过程中的一些条件是随机变量，则属于随机优化问题。这类问题一般也会分解为一系列确定优化问题来求解。

所以本系列我们主要总结 **连续无约束优化问题**。有约束优化和线性规划等问题可能要随缘更新了。

明确我们要讨论的无约束优化问题，下面再针对这类问题做个总结。

## 连续无约束优化问题
一般我们遇到的都是比较复杂的非线性目标函数，无法通过求导数零点的方法找到极小值，所以通过迭代的方法，从某个点出发找到附近的函数值更小的点，再找该点附近更小的点，如此逐步接近最优解。 **那么主要过程就是找当前点附近的更小点。** 有两种思路：

1. 沿着某个方向找到更小点，这需要确定方向和步长，对应线搜索法
2. 在该点的邻域内找到更小点，这需要确定邻域的大小，对应置信域法

而在线搜索法中，如何确定方向，这又有几种方法：

1. 最速梯度法，即沿梯度方向
2. 牛顿法，沿经过黑塞矩阵修正过的梯度方向
3. 拟牛顿法，找一个比梯度好一点的方向就行，计算量不那么大（相比于牛顿法）
4. 共轭方向法，每次沿着与之前都垂直的方向

在置信域法中如何找到更小点，有几种方法：

1. 将问题转化为单值函数近似求解
2. 折线法（dog-leg）
3. 在 $-g$ 和 $B^{-1}g$ 生成的二维空间中搜索


所以本博文的主体将介绍无约束非线性优化方法，大纲就是按照上述分类，分别介绍各种优化方法和收敛速度等。
