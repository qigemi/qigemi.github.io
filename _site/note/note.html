<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title></title>
  </head>
  <body>
    <h1 id="2019425">2019.4.25</h1>
<hr />
<p><strong>PointNet</strong><br />
对三维点云的分类和分割网络，文章讨论了输入点顺序改变时网络性能的不变性，没看懂。</p>
<ol>
  <li>输入$n\times3$矩阵，n为点数，3为x,y,z坐标，经过一个变换矩阵（由T-net生成）将点云变换到相对标准的位置，避免整体transform的影响。</li>
  <li>变换后经过多层感知机（MLP）分别对每个点进行处理，生成$n\times64$ feature，再对feature做（T-net）transform。</li>
  <li>对上述feature再做MLP，max pooling，FC等操作得到全局特征向量，由此可做分类。</li>
  <li>将该向量和$n\times64$ feature concatenate,经过MLP得到每个点的分类。</li>
</ol>

<hr />
<p><strong>FoldNet</strong><br />
基于pointnet的改进，用encoder-decoder结构做无监督学习，可以生成一个点云的特征向量，还可以通过该向量将一个点云变换为另一个。<br />
其中对pooling方法也有改进，他不是全局pooling，而是利用最近邻图（NNG）对某点附近的几个点做pooling（max or average）。</p>

<hr />
<p><strong>On-the-Fly Adaptation of Regression Forests for Online Camera Relocalisation</strong><br />
一种在线训练回归森林做相机重定位的方法。<br />
related works：</p>
<ol>
  <li>图像匹配的方法，对图像生成描述子（词袋或其他向量描述子）</li>
  <li>key points匹配，生成的3D points和当前图像的2D points 匹配（RANSAC，Kabsch等）</li>
  <li>其他方法，如ORB-SLAM结合上述两种方法，用神经网络直接回归6自由度pose，在当前位置附近生成3D points，将其和全局3D points匹配。</li>
  <li>回归森林方法，和词袋类似，需要预先训练一棵回归树，应用时用来做3D-2D匹配。</li>
</ol>

<p>本文的改进是在普遍场景中训练回归森林，然后去掉叶子，应用时online训练叶子。</p>

<h1 id="2019429">2019.4.29</h1>
<hr />
<p><strong>NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences</strong><br />
 一种判断对应点集中内点和外点的方法。 首先根据局部信息找到一些一致性好的相邻点，然后把邻接图输入神经网络（类似FoldNet），输出每个对应点是否是内点。</p>

<hr />
<p><strong>Optimized Contrast Enhancements to Improve Robustness of Visual Tracking in a SLAM Relocalisation Context</strong><br />
<i>本文的直觉是HDR图像中提取的特征点在光照变化时比较稳定，所以通过建立SDR图像不同对比度增强的金字塔，模拟HDR图像的信息。</i>保存多层图像，每层根据互信息做不同的对比度增强，在多层图像中提取特征点。<br />
related works：</p>
<ol>
  <li>SuperFast：用类似反馈的方法动态优化Fast中的亮度阈值，图像每个区域可能有不同的阈值。</li>
  <li>输入HDR图像做色调映射，再提取特征点。对SDR图像主要是改变曝光时间调整对比度。</li>
  <li>对于直接法SLAM一般是在相邻帧之间做灰度放射变换，或者使用降维的深度学习特征，或者用互信息代替光度误差。</li>
</ol>

<p>本文中对每幅图像计算一个金字塔，第一层最大化变换（即对比度增强）后图像与参考图像（人为定义的亮度良好的图像）的互信息。第二层最大化参考图像与条件概率分布的互信息：</p>

<script type="math/tex; mode=display">\boldsymbol{u}^* = \mathop{\arg\min}_{\boldsymbol u} MI(I^{* }, f(I,\boldsymbol u)|f(I,\boldsymbol u^0))</script>

<p>其中MI表示互信息，$f(I,u)$ 为对图像$I$ 做参数为$u(a,b)$ 的灰度映射变换，即灰度小于a的置0，大于b的置1，a,b之间的线性变换到0,1之间。后续层应该类似该过程。</p>

<h1 id="201953">2019.5.3</h1>
<hr />
<h2 id="图像中物体识别和位姿估计刚体且已知3d模型">图像中物体识别和位姿估计（刚体且已知3D模型）</h2>
<h3 id="不同策略">不同策略</h3>
<ol>
  <li>稀疏特征点匹配（描述2D点或点云中的3D点），但是在低纹理场景中使用受限</li>
  <li>生成3D模型，在pose空间中采样重投影，在图像中匹配</li>
  <li>基于稠密点云的方法：1)ICP；2)用所有可能的点对组合，计算对应的pose并投票，可以做detect或计算pose；3)用所有三点的组合，通过随机森林得到一些候选的pose，再通过优化筛选</li>
  <li>局部patch仿射不变描述子，得到相邻patch的邻接关系图模型，用于匹配和重建。（这个忘了在哪看的了，找不到对应论文）</li>
</ol>

<p>还可以分成整体和局部匹配两种策略，也可以同时使用。全局的匹配可能更稳定，很少出现一对多的情况，但是有遮挡情况表现不好。局部特征匹配则对遮挡处理的好，但是会有多意解。</p>

<h3 id="icp">ICP</h3>

<hr />
<p><strong>Fast and automatic object pose estimation for range images on the GPU</strong> (2)<br />
本文即建立3D模型的方法。</p>
<ol>
  <li>用CAD创建物体的3D模型，在位姿空间中采样（在更可能的位姿上更多的采样pose），生成深度图像数据库。用模拟器仿真物体自由下落后出现的位姿，统计直方图，用聚类的方法选择要保存的位姿。</li>
  <li>根据输入的深度图像和数据库估计物体在图像中的大概位置，在该位置上对数据库中的所有位姿计算残差，残差小的作为候选。区域选择方法：深度图中值滤波，提取边缘，计算距离变换，得到可能表示单独物体的像素联通域。根据深度图对每个patch计算平均深度，选大小较大距离较近的几个，用形状中心作为物体坐标系原点，即可得到初始的$T(x,y,z)$。这时候选的patch可能有1-3个。对这几个候选与所有pose计算残差，同时优化Ｔ，得到大概的pose。</li>
  <li>用ICP方法优化位姿。</li>
</ol>

<hr />
<p><strong>Model Globally, Match Locally: Efficient and Robust 3D Object Recognition</strong> (3) <br />
本文为稠密点云策略中的，取点对生成候选pose的方法。</p>
<ol>
  <li>全局描述子：用两个点之间的特征和关系构建一个feature F，计算3D模型中所有两点之间的F，按照F聚类，将相似的F对应的点对通过F的hash索引保存在一起。这样通过图像中的点对的F即可找到模型中对应的可能的点对。</li>
  <li>在图像中找一个参考点 $s_r$，遍历所有其他点构成点对 $(s_r,s_i)$，计算F，在全局模型中找到相似的点对对应 $(m_r,m_i)$，计算每组对应的transform，在空间 $(m_r,\alpha)$ 中投票。最终得到参考点对应的最可能的几个 $m_r,\alpha$，可据此计算出transform。</li>
  <li>由于上述计算前提为s是物体表面上的点，所以要多取几个s，保证有点在表面上。 得到了这么多可能的pose之后，对他们聚类，每类的得分为类内所有pose的得分，即上述投票步骤中所获得的票数。最后得分高的几个类，类内pose平均后输出。</li>
</ol>

<hr />
<p><strong>Global Hypothesis Generation for 6D Object Pose Estimation</strong> (3)<br />
本文即稠密点云策略中，取三点生成pose的方法。</p>
<ol>
  <li>在一定范围内取三点，通过随机森林判断图像点属于哪个物体及在物体上的坐标，进而计算出pose</li>
  <li>不同于常规的pose采样方法（RANSAC），本文用一个全链接的条件随机场判断像素是否是物体上的点，这样可以减小搜索空间</li>
  <li>最后用ICP优化。</li>
</ol>

<h1 id="2019510">2019.5.10</h1>
<hr />
<h2 id="三维重建专题">三维重建专题</h2>
<p>related works</p>
<ol>
  <li>由带有标注的2D图像恢复3D模型</li>
  <li>三维监督学习</li>
  <li>多视图重建</li>
  <li>多图像训练，单视图深度估计</li>
</ol>

<p><em>总结来说，要从有限信息中恢复三维信息，必须要有先验知识，比如有统计模型或者标注等。</em></p>

<hr />
<p><strong>What Shape Are Dolphins? Building 3D Morphable Models from 2D Images</strong>（1）</p>

<p>从单图像恢复物体的三维模型。注意只能针对一个类型的物体，因为要生成该类物体的3D模型。</p>
<ol>
  <li>从n幅图像中生成3D模型，包括一个平均模型和几个basis变形的模型（由PCA生成）。</li>
  <li>来一个新图像时需要提供图像中物体的轮廓，和轮廓上特征点和3D模型上的对应。</li>
  <li>优化目标函数为：通过模型模拟生成的轮廓与输入轮廓的匹配误差，控制点的匹配误差，模型平滑和正则化，3D模型上对应于2D轮廓的曲线的连续性；优化参数包括：优化basis模型的组合系数，相机外参，微调模型上的控制点。</li>
</ol>

<p>文中还介绍了优化的方法，如何找到符合2D图像的轮廓，subdivision surface model等。</p>

<hr />
<p><strong>Single Image 3D Interpreter Network</strong>(1)</p>

<p>This is a learning based method.Eevery category has a set of basis skeletons which is manually designed. <em>This method do not need annotations in input image, while the express of object is sparse.</em> Use a detection net beforehand so we know the category of object in input image.</p>
<ol>
  <li>keypoint estimate from input image, and output a heat map of keypoints. With a fine tune approach.</li>
  <li>3D Interpreter. find $P,R,T,{\alpha_k}$ to fit:</li>
</ol>

<script type="math/tex; mode=display">X = P(R\sum_{k=1}^K \alpha_kB_k+T)</script>

<p>where X is points in image, $B_k$ is basis skeleton which are coordinates of 3D points.</p>

<ol>
  <li>project the final skeleton to image and get 2D points.</li>
</ol>

<p>这个方法类似于pnp，不过特征点提取和匹配过程用网络生成heat map代替。</p>

<hr />
<p><strong>Reconstructing PASCAL VOC</strong> (1)<br />
该方法的输入图像需要标注物体的分割，及一些控制点，控制点数量大于上一篇论文的方法，所以重建出来的模型比较稠密。</p>
<ol>
  <li>相机姿态估计。对每个类标注分割和几个控制点，因为物体非刚性，取其中可能不变的部分用SFM生成该类的刚体模型。根据相机投影成像公式，用迭代的方法求相机姿态。</li>
  <li>通过轮廓误差细调相机位姿</li>
  <li>将所有的相机视角聚类，用PCA选出三个主方向，每个主方向相差15度的实例的轮廓作为该主方向上的轮廓候选之一。在重建时，根据每个主方向上实例的多少，按概率抽选两个主方向中的两个轮廓作为三视图中的两个视图，与输入图像的轮廓组合成三视图。选择多次生成多组三视图。</li>
  <li>用三视图重建3D模型，注意这里用的不是同一个实例的三个视图，所有重建会有很大误差。生成多组3D模型，根据模型在三个主方向上的投影，和每个方向上平均投影的相似度进行打分，相似度高的胜。</li>
</ol>

<hr />
<p><strong>3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction</strong>（2）<br />
3D Recurrent Reconstruction  Neural  Network<br />
用RNN网络记住之前输入的图片，实现单图片到多图片兼容的三维重建，图片越多重建越准确，也有图片多了失败的。输入图片只需要标注bounding box。<br />
细节：</p>
<ol>
  <li>一张或多张图片输入LSTM，将隐藏层中的特征拿出来经过一个decoder，做3D反卷积，生成occupied confidence map</li>
  <li>训练时每个patch用相同数量的图片输入，测试时可以每输入一个图片就输出一个结果。</li>
</ol>

<hr />
<p><strong>Learning a Predictable and Generative Vector Representation for Objects</strong>（2）<br />
先用3D模型训练一个3D模型的encoder-decoder网络，得到3D模型的特征向量F。再训练一个2D图像的encoder，使其生成F。测试时从2D图像经过encoder生成F，再由F经过3D decoder得到3D模型。</p>

<hr />
<p><strong>Convolutional Mesh Regression for Single-Image Human Shape Reconstruction</strong><br />
事先有一个人体的mesh模型，通过CNN提取图片中人体特征，将该特征和mesh模型（一些3D控制点坐标）结合，经过graph CNN，回归出mesh中3D点的坐标，恢复图片中的人体模型。</p>

<h1 id="2019515">2019.5.15</h1>
<hr />
<h2 id="恢复三维人体或者pose">恢复三维人体（或者pose）</h2>
<p>大部分方法也需要先验模型。机器学习方法中有分步估计（shape and pose），或者end-to-end方法。目前最好的建模方法是SMPL(A skinned multi-person linear model)。</p>

<ol>
  <li>对图像中的人体关键点计算其3D坐标(pose)<br />
1.1 提取关键点后估计每个点的深度<br />
1.2 根据关键点建立basis model，线性组合成模型后投影回图片，最小化残差<br />
1.3 由2D点之间的信息恢复3D点之间的信息</li>
  <li>重建稠密3D模型(shape)<br />
2.1 类似上面说的海豚重建，通过统计构建通用模型，然后根据输入的轮廓信息，估计模型参数调整模型</li>
  <li>Pose &amp; Shape，同时估计pose和shape</li>
</ol>

<hr />
<p><strong>SMPL</strong><br />
<script type="math/tex">M(\beta,\theta;\Phi):\mathbb R^{|\theta|\times |\beta|}\Rightarrow\mathbb{R}^{3N}</script><br />
$\beta$ stands for shape param, $\theta$ stands for pose param.</p>

<hr />
<p><strong>Ordinal Depth Supervision for 3D Human Pose Estimation</strong>（1.1）<br />
先经过encoder-decoder结构生成人体关键点和对应的深度，再经过一个网络生成每个关键点的三维坐标。其中深度估计网络的训练用到了两个关键点之间的深度关系，即ordinal depth，具体的点对顺序和选择是固定的，不需要所有点对组合，只取其中的一部分点对组合。</p>

<hr />
<p><strong>3D Human Pose Estimation from a Single Image via Distance Matrix Regression</strong>（1.3）<br />
计算图片中关键点两两之间的距离，组成矩阵，经过一个full connect 和一个full conv生成3D关键点两两距离矩阵，根据这个矩阵通过一个半定规划优化方法，解出3D模型。</p>

<hr />
<p><strong>Learning to Estimate 3D Human Pose and Shape from a Single Color Image</strong>（3）</p>
<ol>
  <li>图像经过关键点点检测提取关键点，关键点位置经过网络生成pose param</li>
  <li>图像经过分割网络得到轮廓，经过网络生成shape param</li>
  <li>pose and shape param组合生成3D模型，重投影回图片，根据点的残差和mask误差，优化网络</li>
</ol>

<hr />
<p><strong>End-to-end Recovery of Human Shape and Pose</strong>（3）</p>
<ol>
  <li>图片经过encoder得到编码，由编码回归出SMPL模型参数，该过程是迭代多次的（因为单次回归不准确）</li>
  <li>回归网络训练的时候可以有3D标注，也可以没有。loss包括3D关键点误差，3D模型误差，关键点重投影误差和模型参数误差。但是注意每个迭代都用ground truth监督的话可能会陷入局部极小值，所以只用了重投影误差和3D误差训练</li>
  <li>因为可能没有3D ground truth，模型可能学习出奇怪的结果，尽管其2D投影是对的。所以最后增加一个对抗先验网络，判断生成的模型是否是真实的人体模型。every joint has a discriminator, 每个分类器有loss，最后总体有loss，同时训练（这里没细看）。</li>
</ol>

<p>总体loss</p>

<script type="math/tex; mode=display">L = \lambda (L_{reproj}+L_{3D})+L_{adv}</script>

<hr />
<p><strong>Exploiting temporal context for 3D human pose estimation in the wild</strong><br />
作者Andrew Zisserman，在同时估计pose &amp; shape的基础上，对多帧图像中的人体做BA，完善了结果，并且用该方法给大量网络视频做了标注，实验表明，其他算法在这些标注上训练后效果变好了。</p>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  </body>
</html>
