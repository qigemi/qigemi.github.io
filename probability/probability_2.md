---
layout: post
---

这一部分会介绍一些特殊的**随机过程**。我们之前讨论了随机变量，而实际应用中很多情况我们不会只在一个时刻讨论随机变量，
而是随着时间的进行，在不同的时刻都有一个随机变量，也就是随机过程。当然根据时刻的连续和离散也分为连续随机过程和离散随机过程。

对于离散随机过程，我们用伯努利过程作为例子。连续随机过程用泊松过程作为例子。这两个分布都是每个时刻之间相互独立的。然后介绍有时间相关性的马尔可夫过程。

首先从相对简单的离散随机过程开始。

# 1.伯努利过程

伯努利过程，就是一连串相互独立的伯努利试验。而伯努利实验就是每次实验有两个结果，不是成功就是失败，每次成功的概率 p 都相同。

这里有两个关键假设，一是每次实验相互独立，二是每次实验成功的概率相同。

那么有哪些场景可以应用伯努利过程建模呢。比如扔硬币，连续抽奖，或者是统计一段时间内某件事发生的次数。
比如统计一段时间内有电话打进来的次数，可以将时间分成小段，并假设每段时间内来电话的概率相同。或者一段时间内商场里进入的人数。

> 对于离散随机过程，我们可以认为是一系列随机变量的集合，也可以将其视为一个整体，看作一个随机变量。
以伯努利过程为例子，如果将其视为整体，则采样空间中的元素就是这个样子：（010011000110……，11001001……，……）。每个元素都是一串1或0，表示随机过程中每次实验的结果成功或失败。

然后我们来分析一个例子。将n个时间段内一共发生的事件数S视为一个随机变量，用伯努利过程来建模。

$$P(S=k)=C_n^k p^k(1-p)^{(n-k)}\\
E[S]=np \\
var(S)=np(1-p)$$

然后我们反过来分析，即发生S次成功，需要多少次实验。比如用 $T_1$ 表示需要多少次实验使实验第一次成功。这是一个几何分布。

$$P(T_1=t)=(1-p)^{t-1}p\\
E[T_1]=1/p\\
var(T_1)=(1-p)/p^2$$

这些都是之前讨论过的内容。

对于独立性我们可以这样认为，即已经发生的结果不会影响之后我们对实验结果的判断。无论我们在伯努利过程的什么时刻介入观察，我们都将看到一次独立的伯努利试验，和之前的结果没有关系。
但是如果有一个预言家，他知道未来几次的实验结果，那我们看到的就不是独立的随机实验了，因为结果已经定了。

那么在第一次成功之后，再经历多少次实验会再次成功呢？因为过去的结果不会影响后面的实验，所以在第一次成功后，我们相当于开始了一个新的伯努利过程，所以再次成功随需要的次数
也是一个几何分布的随机变量，且这两个随机变量相互独立。

所以S次成功所需要的试验次数作为一个随机变量，就等于S个相互独立的、服从几何分布的随机变量相加。

# 2.泊松过程

泊松过程就是伯努利过程的连续版本。比如在连续的时间上，随时都有可能发生某个事件，或者说在任意时间发生的概率相同。它也有两个假设和伯努利过程对应。

一是定义 $P(k,\tau)$ 为时间间隔 $\tau$ 内发生事件的次数为k的概率。这个概率只和时间间隔大小有关，而与所取的区间的绝对位置无关。
即无论在哪个时刻，取相同大小的时间间隔，其中发生事件次数的概率分布是一样的。

第二个假设也是独立性，即不交叉的时间间隔内，事件次数的分布是独立的。

对于无穷小的时间间隔有如下假设：

$$P(k,\delta)\approx \left\{ \begin{align}
1-\lambda \delta,\ & if\ k=0,\\
\lambda \delta,\ & if\ k=1,\\
0,\ & if\ k>1.
\end{align}\right.$$

也就是说，在无穷小的时间间隔内可以认为是一次伯努利试验。那么 $P(k,\tau)$ 的解析解，可以通过将 $\tau$ 分割成n个无穷小间隔，然后用伯努利过程计算。
最后求n趋向无穷大时，P的表达式。最后的结果是：

$$P(k,\tau)=\frac{(\lambda \tau)^ke^{-\lambda \tau}}{k!}\\
E[N_t]=\lambda t,\ var(N_t)=\lambda t$$

在没有更多假设的情况下，任何随机到来的事件（即可能在任何时刻发生），都适合用泊松过程来建模。
比如一段时间内收到了多少邮件，一个光源发射了几个光子，粒子衰变等等。

同样我们再来计算一下第k次事件发生所需要的时间 $Y_k$，这个随机变量的概率密度函数，当然这是一个连续随机变量。

我们这样分析，如果发生k次需要时间为y，则需要在最后的 $\delta$ 时间内发生一次事件，同时满足在之前 $y-\delta$ 时间内发生 k-1次。
当 $\delta$ 足够小时这个概率可以表示为：

$$f_{Y_k}(y)\delta =P(y\leq Y_k\leq y+\delta)\\
=P(k-1,y)\cdot \lambda \delta\\
=\frac{\lambda (\lambda y)^{(k-1)}e^{-\lambda y}}{(k-1)!}\cdot \delta$$

对于不同的k，该分布有不同的形状，当k=1时的形式比较简单：

$$f_{Y_1}(y)=\lambda e^{(-\lambda y)}$$

也就是说泊松过程中事件第一次发生的时间服从一个指数分布。

回忆一下，在伯努利过程中，第一次事件发生所需要的实验次数服从几何分布。实际上几何分布就是指数分布的离散形式。

泊松过程也有时间上的独立性，过去的实验结果不会影响未来的概率分布。

### 例一

一个有趣的例子。有三个一样的灯泡，它们坏掉的时间服从参数为 $\lambda$ 的指数分布，求最后一个灯泡坏掉所需要的时间的期望。

这里利用了泊松过程的融合，灯泡坏掉可以认为是一个泊松过程中第一次事件发生。
那么同时等待三个灯泡坏掉，就相当于事件有三倍的概率发生，三个灯泡中第一个灯泡坏掉的时间也就服从参数为 $3\lambda$ 的指数分布，
这个时间的期望是 $1/3\lambda$。当第一个灯泡坏掉后，就剩下两个灯泡，则再有一个坏掉的概率是一个灯泡的两倍。
也就是说从第一个灯坏掉到第二个灯泡坏掉所需要的时间，服从参数为 $2\lambda$ 的指数分布。以此类推，最终三个灯泡都坏掉所需时间的期望为
 $1/3\lambda+1/2\lambda+1/\lambda$。

### 例二
另一个更有趣的例子。公共汽车到来可以认为是泊松过程，巴士公司声称车辆到达的频率是每小时四辆，则两车之间的时间间隔期望是15分钟。
然后你就去调查是否真的是15分钟。方式如下，当你到达车站时问在那的人，上一辆车是什么时候来的，然后坐等下一辆车来，将这两个时间相加。

因为泊松过程在时间上的独立性，第二段时间的期望就是15分钟，不受之前所发生事情的影响。而第一段时间，可以认为是在时间上反向发生的泊松过程，那么其期望也是15分钟。

你会发现相加的结果是30分钟。为什么呢？时间间隔的期望不应该是15分钟吗？
上面的推导并没有问题，问题在于求期望的过程。期望可以认为是加权平均，而关键就在于我们对什么取了平均，或者说在哪个集合中采样得到的平均。

正常来说要求两车之间的间隔，我们要随机选择的是车辆，在所有车辆中随机的选择一个，然后统计它和下一辆车之间的间隔，再取平均。
但是在这个例子中，我们实际选择的是“时间间隔”。
因为我们是在随机的时间到达车站，相当于随机选择了一个“间隔”，然后统计这个间隔的时间，再求平均。显然更大的时间间隔被选中的概率更大，所以最终的结果偏大也就合理了。

这个例子说明我们在做统计或者看统计结果的时候，应该注意一下统计的范围是什么，否则会导致结论有偏差。

比如我们要调查北京地铁人多吗？我们应该随机的选择车辆，然后数人数，而不应该随机的选择一个人，问他你那辆车上人多吗。
假如只有两辆地铁，一辆全是人，另一辆没有人，那么使用第二种方法，所有人都会说我的车很挤。这个结果是有偏差的。

# 3.马尔可夫过程

前面讨论的是时间上不相关的随机过程，马尔可夫过程是依赖于系统当前状态的过程。

首先用离散的情况举个例子，比如超市的顾客排队结账，在离散的时间上，每个时刻有可能再来一个人，也可能没有人来，这个事件可以认为是伯努利实验。
每个人结账的时间也不一样，是一个随机变量，可以简单的等价为每个时刻可能少一个人（当前顾客结完帐了），也可能不少，也是一个伯努利实验。

在这种情况下如果我们要问半小时后队伍会有多少人，则这个分布是和当前的人数相关的。我们可以用当前人数作为这个系统的状态描述。
一开始状态可能是0，则下一个状态可能是1，或者保持0。如果某个时刻状态为n，则下一时刻可能是n-1,n,或n+1。

系统在每个时刻的状态就构成了马尔科夫链，也叫马尔可夫过程。系统从当前状态会以某种概率转移到另一个状态，或维持当前状态，而且转移概率只和当前状态有关。
当我们知道所有的状态转移方式和概率后，就可以从任意状态开始，分析系统未来的状态。

下面是比较严格的定义。

## 3.1有限状态的马尔可夫过程

首先简单起见，我们的时间是离散的，系统状态是有限的。定义 $X_n$ 为系统在时刻n的状态，$X_0$ 为初始状态。定义：

$$p_{ij}=P(X_{n+1}=j\vert X_n=i)\\
=P(X_{n+1}=j\vert X_n=i,X_{n-1},\cdots ,X_0)$$

第一个等式是定义 $p_{ij}$ 表示当前状态是i的情况下，下一个状态是j的概率。第二个等式的意思是，这个概率只和当前状态有关，而和过去所有的状态无关。
不管系统是怎么到达当前状态i的，从i到j的概率都是一样的。这是马尔可夫过程最重要的假设，也叫**马尔可夫性质**。

这样看来马尔可夫过程也有时间上的独立性，它只和当前状态相关。我们要小心的选择系统状态，保证所有影响系统转移的因素都包含在状态中，
否则我们无法推算系统下一步的状态。这又和卡尔曼率波的状态转移有点像。

用马尔可夫过程分析问题分三步，
1. 选择表示系统状态的方式；
2. 列出所有状态转移的方式；
3. 计算每个状态转移的概率。

有了模型之后我们就可以预测系统未来几步可能处于什么状态。

马尔可夫过程有一个性质，就是不管初始状态是什么，经过足够长的时间后，系统会到达一个"稳态"，虽然系统的状态仍然在跳转，但是系统处于各个状态的概率将保持稳定。
这个性质在大部分情况下成立。但是有一些情况是不成立的，比如马尔科夫链中有周期性的结构。下面来具体定义这些结构。

下面的部分最好能够结合图形来理解，文字表述不太清楚，所以我记录的比较简单。

### 可重复状态

系统的状态可以分为两种，
1. 可重复的（recurrent），即从该状态出发，不管怎么走最终都能走回来，我们记为R
2. 暂时的（transient），不属于1的状态即称为暂时的状态，我们记为T。

如果系统从T出发，则经过有限的步骤，系统一定会跳出T的状态，进入一个R的集合。或者说在足够长的时间后系统不会处于T的状态。

而R的集合定义为可以相互到达的R的状态的集合，两个集合中的状态不能相互到达，即如果进入了某个R的集合，则再也不会到达该集合之外的状态，系统被“困”在这个集合中。
所以在有多个R集合的结构中，初始状态将会影响系统最终的状态。而如果只有一个R集合，则无论初始状态如何，系统都将落入这个集合。

### 周期性结构

周期性结构定义在R集合中，如果R集合中的状态可以分成至少2组，使得每组中的状态经过一步转移后都会进入固定的下一组状态，则称这个集合有周期性结构。

结合上面两个定义，如果系统只有一个R集合，则初始状态不会影响系统的最终状态。如果该R集合中没有周期性结构，则系统最终会到达稳态，即处于每个状态的概率会收敛到稳定值。
这就是**马尔科夫链的稳态收敛定理**。

## 3.2 birth-death process

下面讨论一种简单但是重要的马尔可夫过程，就是我们上文提到的超市结账的例子。对于有很多状态的马尔可夫过程，要求稳定收敛的概率很困难，
但是对于这种birth-death过程来说就很容易计算。

这种系统的状态用自然数表示，从0到有限的n。每次系统状态只会加1或减1或者不变。
记 $P_{i,i+1}=p_i,P_{i+1,i}=q_{i+1}$， 则 $P_{i,i}=1-p_i-q_i$ 。

该模型适合建模“任务队列”这种情况。也适用于人群感染病毒的情况，感染人数可能增加也可能有人痊愈，且人数增加的概率会随着感染人群的增加而增加。

我们如何求该系统的稳态概率呢？有一个trick，还是建议看视频中的图。https://www.bilibili.com/video/BV19s41167TE?p=59。

我们考虑两个相邻的状态，2和3，从中间将系统状态分为两份，
则这两份之间只有两条通路链接，即2-3和3-2。系统走过这两条路的次数必定最多只差1次，也就是说这两条路被走过的概率相等。得到如下等式：

$$\pi_ip_i = \pi_{i+1}q_{i+1} $$

这样结合 $\sum \pi_i=1$ 我们就可以解出所有状态的稳态概率。
从这个公式我们也可以直观的理解，如果p大于q，则系统更容易向大的方向转移，所以数字大的状态的稳态概率会更高。

对于特殊情况，$p_i=p,q_i=q$， 所有的p都相等，q都相等，有：

$$\pi_i=\pi_{i+1}q/p=\pi_{i+1}\rho\\
\pi_i=\pi_0\rho^i\\
\pi_0=1/(\sum_{i=0}^m \rho^i)$$

如果p=q，则所有状态的稳态概率都相等，这时系统叫做对称随机游走过程，在足够长的时间后系统处于每个状态的概率相同。

# 4.极限定理（limit theory）

我们之前讨论了如何分析1个或两个随机变量的概率问题，这种问题比较容易用概率密度函数等等方法来分析。
当变量的数量增加到成百上千的时候，问题就不那么容易解决了。幸好当变量数量足够多的时候，某些概率问题的结果会收敛（像马尔科夫链的收敛过程），让这个问题变得容易解决。
这就需要依靠一些“极限定理”。

首先从**马尔可夫不等式**开始。假设有离散随机变量 $X \geq 0$，则有

$$E[X]=\sum xp(x)\\ \geq \sum_{x\geq a}xp(x)\\ \geq \sum_{x\geq a}ap(x)\\
=aP(X\geq a)$$

因为X是非负的，所以对于 $X \geq a$ 的部分求和一定小于等于全部的和，所以第二个式子成立。
注意第三个式子需要 $x\geq a$ 时才成立。最后的结果表示，如果X的期望很小，则X取较大值的概率也很小。

对于下面的式子

$$E[(X-\mu)^2]\geq P((X-\mu)^2\geq a^2)a^2\\=P(\vert X-\mu\vert \geq a)a^2$$

则表示，如果X的方差很小，则X偏离期望的概率也很小。

然后是**切比雪夫不等式**，推导原理和上述一样，不过是连续随机变量。假设连续随机变量X，真实的均值和方差为 $\mu,\sigma ^2$

$$\sigma ^2=\int (x-\mu)^2f(x)dx\\
\geq \int_{\infty}^{\mu-c} (x-\mu)^2f(x)dx + \int_{\mu+c}^\infty (x-\mu)^2f(x)dx\\
\geq c^2P(\vert x-\mu\vert \geq c)\\
\Rightarrow P(\vert x-\mu\vert \geq c)\leq \frac{\sigma^2}{c^2}\\
P(\vert x-\mu\vert \geq k\sigma)\leq \frac{1}{k^2}$$

也就是说X偏离均值的程度越大，概率越小。比如偏离 $3\sigma$ 的概率小于等于 1/9。注意这是对任意随机变量来说的，不只是高斯分布。

有了上面两个不等式，我们再来讨论一下极限。首先定义数列的极限，也就是数列收敛到某个数。这个定义就不赘述了。

那么随机变量是怎么收敛到一个数的呢。和数列极限类似，对于任意小的 $\epsilon$：

$$\lim_{n\rightarrow \infty}P(\vert Y_n-a\vert \geq \epsilon)=0$$

Yn是一系列随机变量的数列，可以认为是随机过程，最后Y收敛到a，就是说最后随机变量Y的值，不在a附近任意小区间的概率为0。

注意：随机变量收敛到a，并不表示期望收敛到a。也不表示方差收敛到0.

## 4.1弱大数定理

接下来我们看采样均值是否收敛于真实的均值。随机变量Xi为随机变量X的第i次采样，则它们服从同样的分布。
假设真实的均值和方差为 $\mu,\sigma ^2$，则n次采样的均值也是一个随机变量

$$M_n=(X_1+...+X_n)/n\\
E[M_n]=\mu n/n=\mu\\
var(M_n)=n\sigma ^2/n^2=\sigma ^2/n$$

采样均值的期望和真实期望相同，但是方差会随着采样数量的增加而减小。套用切比雪夫不等式：

$$P(\vert M_n-\mu \vert \geq c)\leq var(M_n)/c^2=\frac{\sigma ^2}{nc^2}$$

用另一种说法就是，Mn在概率上收敛于 $\mu$。这就是弱大数定理。

当我们没有办法计算真实的均值的时候，我们只能通过采样计算样本均值。
当采样数量足够多的时候，我们就可以认为样本均值在一定概率上误差是足够小的。通过设定我们能接受的误差和置信度，我们可以反算需要的采样数量多少。

## 4.2 中心极限定理

采样均值是通过求和后除以n得到的，Mn均值收敛于真实均值，而方差不断变小。那么如果求和后除以 $\sqrt{n}$，方差就会保持一个常数。
假设Xi是独立同分布的，定义

$$S_n=X_1+...X_n\\
Z_n=\frac{S_n-E[S_n]}{\sigma_{S_n}}=\frac{S_n-nE[X]}{\sqrt n \sigma}$$

这叫做随机变量的标准化（standardize）。Zn是均值为0，方差为1的随机变量，可能是任意分布。
但是当n足够大时，Zn的累积分布函数会收敛到标准正态分布的累积分布函数：

$$P(Z_n\leq c)\rightarrow P(Z\leq c)$$

其中Z服从标准正态分布N(0,1)。这就是中心极限定理。

中心极限定理有三点好处：

1. 无论X服从什么分布，只要有真实的均值和方差就可以应用该定理
2. 计算方便且足够精确，只要查标准正态分布表就行
3. 很符合实际应用场景，当噪声影响因素很多的时候，用高斯分布就可以很好的建模

注意这里是CDF收敛到标准正态分布，而不是PDF或PMF，它们可能是不一样的。

在实际中我们不能让n趋于无穷，实际上，当n=15的时候，这种近似就相当好了。这可能也是我们做实验的时候要重复测量15-20次的原因。

下面看一个例子来体会中心极限定理近似的程度。假设Xi服从p=0.5的伯努利分布，n=36，即Sn为36个独立的伯努利结果相加，求 $P(S_n\leq 21)$

用中心极限定理计算为：

$$E[S_n]=np=18\\var(S_n)=np(1-p)=9\\
P(S_n\leq 21)=P(\frac{S_n-18}{3}\leq \frac{21-18}{3})\\
=P(Z\leq 1)=0.843$$

用解析方法计算精确结果为：

$$\sum_{k=0}^{21}(\begin{matrix}36 \\ k\end{matrix})(1/2)^{36}=0.8785$$

这个结果比较精确，但是并不十分精确。有提升精度的方法。因为我们在考虑离散的分布，

$$P(S_n\leq 21)=P(S_n < 22)$$

这两个概率是一样的，所以我们可以取中间的值，即计算

$$P(S_n \leq 21.5)=P(Z\leq 1.17)=0.879$$

可以看到这个结果就和准确数值非常接近了！利用这种加0.5的方法，还可以计算Sn=21的情况，因为近似的标准正态分布是连续分布，不能计算等于某个值的概率。
实际上这个trick是先于中心极限定理产生的，当时就是为了计算二项分布的概率，发现了其极限是标准正态分布，进而推广到所有分布的情况。

下面有一个错误的推理，我们来思考一下是哪里有问题。在0-1的时间内事件发生服从泊松分布。我们可以将时间分成n个小段，每个小段无穷小时近似为伯努利分布，
且每段之间是独立的。这是我们之前分析泊松分布的时候做的假设。那么当我们将这n个伯努利分布相加的时候，如果n趋于无穷，其和应当服从什么分布？是高斯分布吗？不应该是泊松分布吗？

这里的误区在于，中心极限定理还有一个隐含的条件，就是Xi要保持相同的分布。而这个例子中当n变大的时候，Xi服从的伯努利分布参数会发生变化，所以不能应用中心极限定理！

这里给了我们一点启发，在实际应用中，如果二项分布(n,p)，p固定，n趋于无穷时可以近似为高斯分布，np固定，n趋于无穷时近似为泊松分布。
再具体一点，当np大于20的时候，我们可以将其近似为高斯分布，np较小的时候可以近似为泊松分布。

# 5.统计推断 statistics inference

我们从真实世界中采集数据，通过分析数据的规律，来给真实世界建模，甚至去推测没有观测到的结果。这就是统计推断的过程。

很多领域应用到统计的方法，比如分析一种药物是否有效，预测金融的发展，根据人们的浏览记录来推荐他们感兴趣的东西，
工程上根据有噪声的观测去预测系统实际的状态等等。

和我们之前学习的概率基本知识不同的是，概率问题属于数学问题，它有唯一正确的解。而统计更像是工程问题，同样的问题，
每个人可能选择不同的方法来建模，有些方法可能看起来正确，但实际上却存在偏差，导致结果差之千里。

推断问题可以分为这么几类：

1. $X=aS+W$，S是输入，a是系统模型，W是噪声，X是观测，如果X，S已知，求a，这叫对系统建模；
如果X，a已知，求S，这叫带有噪声的未知量估计
2. 假设检验：未知量取少数可能值中的一个，因为取错误的值的概率较小，这样剩下的就是正确值
3. 估计：目标是使某个连续变量的估计误差最小

基本上推断问题都包含在上面2.3两种情况中了。

还可以用另一种分类方式：

1. 经典概率 classical：经典概率认为未知的参数是确定的实数，比如电子质量，是唯一确定的数，不是随机变量。
则它的观测的分布是参数的函数 $P_X(x;\theta)$，通过分析该分布求参数的估计 $\hat \Theta$。
2. 贝叶斯概率 Bayesian：认为未知的参数是一个随机变量，即使是电子质量，我们不认为是确定的值，
只不过因为我们之前的一系列测量结果，我们对参数的分布有一个先验。
则它的观测分布是在已知参数先验分布情况下的条件概率 $P_{X\vert \Theta}(x\vert \theta)$。通过该分布求参数的估计 $\hat \Theta$。

注意，虽然两种方法的假设不同，但最后参数的估计 $\hat \Theta$ 都是一个随机变量。概率发展至今，哪种方法更加正确始终没有定论。
我们会先讨论贝叶斯估计，然后是经典估计。

## 5.1 贝叶斯估计

利用贝叶斯公式：

1.假设检验：

$$p_{\Theta \vert X}(\theta\vert x)=\frac{p_{\Theta}(\theta)p_{X\vert \Theta}(x\vert \theta)}{p_X(x)}$$

首先已知参数的先验分布，以及给定参数的情况下，各种观测的分布函数。这是分子的部分。
然后根据这些概率求和就得到分子的部分。最后就可以得到给定观测X的情况下，参数的后验概率分布。

2.变量估计：就是上述公式的连续形式。

贝叶斯估计中的先验分布很重要，它会影响参数的后验分布。
比如抛硬币正面向上的概率，这个先验可以认为是0-1之间的均匀分布，也可以认为是以0.5为中心的高斯分布。两种假设得到的结果将会不同。

无论连续还是离散的情况，最后我们都会得到参数的后验分布。如果一定要给一个数，参数到底是多少呢？有几种不同的方法。

1. 最大后验概率（MAP），找到使后验概率最大的参数数值作为估计
2. 条件期望，即后验概率的期望作为估计

两种估计显然是不一样的，且都没有很强的说服力，除非我们提出评判标准才能判断那种结果更好。
但无论如何点估计都无法很好的表示参数的分布情况。下面介绍几种评判标准。

### 最小二乘（least mean square LMS）

也就是最小化平方误差 $E[(\Theta - c)^2]$，c是参数的估计。最小化该式，使其导数等于0：

$$E[\Theta^2]-2cE[\Theta]+c^2\\
\rightarrow c=E[\Theta]$$

最小二乘的最佳估计就是期望。那么用期望作为估计，在最小二乘的标准下，误差是多少呢

$$E[(\Theta - E[\Theta])^2]=var(\Theta)$$

就是参数的方差。

对于有观测 $X=x$ 的情况，就是条件概率，最小化 $E[(\Theta - c)^2\vert X=x]$ 的结果是 $c=E[\Theta\vert X=x]$。
对于每一个 $X$ 的取值，上式都是最优估计，所以 $E[\Theta\vert X]$ 就是所有X的函数中，对参数的最优估计。

总结一句话，**贝叶斯最小二乘估计就是条件期望。** 它是X的函数。

$$\hat \Theta=E[\Theta\vert X]$$

估计误差：$\tilde \Theta=\hat{\Theta}-\Theta$，是一个随机变量

$$E[\tilde \Theta]=0\\
E[\tilde \Theta\vert X=x]=0$$

这表示参数的估计误差有时是正的有时是负的，但总体上误差为0，所以又叫**无偏估计**。
还有几个性质

$$E[\tilde \Theta h(X)\vert X]=0\rightarrow
E[\tilde \Theta h(X)]=0\\
cov(\tilde \Theta, h(X))=0\rightarrow cov(\tilde \Theta, \hat \Theta)=0$$

h(X)是X的任意函数。上述公式都可以通过以前学过的知识推导出来。
还有一条方差：

$$\Theta=\hat{\Theta}-\tilde \Theta\\
\rightarrow var(\Theta)=var(\hat{\Theta})+var(\tilde \Theta)$$

### 线性最小二乘

假如我们用线性方程作为参数的估计，即 $\hat \Theta=aX+b$，
然后最小化 $E[(\Theta-aX-b)^2]$，求最优的a和b，得到的结果是

$$\hat \Theta=E[\Theta]+\frac{cov(X,\Theta)}{var(X)}(X-E[X])$$

这是一个线性函数，表示了估计值和X的关系。首先如果没有观测X，参数有一个自己的先验分布得到的期望。
然后如果X偏离了均值，则参数会向相应的方向偏离均值（根据协方差的大小和正负）。这可能就是协方差表示**线性相关性**的原因。

线性最小二乘的误差为

$$E[(\hat \Theta-\Theta)^2]=(1-\rho^2)\sigma^2_{\Theta}$$

参数的方差越大，估计误差越大；参数和X的相关性越大，估计误差越小。
可以这样考虑，如果两者完全相关，则给定X即可唯一确定参数，所以估计误差为0；
如果两者完全不相关，则X不会给出任何信息，估计的误差就是参数本身的方差。
只要两者相关，X就会给出参数的一些信息，就会使估计误差变小。

如果观测量是参数加上噪声，且参数和这些噪声独立：

$$X_i=\Theta+W_i\\
\Theta \sim \mu,\sigma_0^2, \ W_i \sim 0,\sigma_i^2$$

则线性最小二乘结果为：

$$\hat \Theta=\frac{\mu/\sigma_0^2+\sum_{i=1}^nX_i/\sigma_i^2}{\sum_{i=0}^n1/\sigma_i^2}$$

就是每个观测量的加权平均，其观测误差方差越大的，权重越小。
**当所有分布都是正态分布的时候，该估计等价于最优最小二乘估计**。

在某些情况中我们也可以求比如 $X^3$ 的线性函数，或者其他X的函数的线性函数。

## 5.2 经典统计推断

经典理论认为待估计的参数是确定的数，不是随机变量，用小写 $\theta$ 表示。具体问题包括：

1. 假设检验，参数只取几个可能的值
2. 混合检验，比如 $H_0:\theta=1/2,\ H_1:\theta \neq 1/2$
3. 估计：设计一个观测X的函数作为参数的估计 $\hat \Theta$，最小化 $\hat \Theta-\theta$，这是一个随机变量。

### 极大似然估计（maximum likelihood estimation）

因为观测X是和参数有关的，所以一个想法是，选择一个参数，使得当前观测X出现的概率最大

$$\hat \theta_{ML}=arg \max_{\theta}p_X(x;\theta)$$

对比贝叶斯最大后验概率估计：

$$\hat \Theta_{MAP}=arg\max_{\theta}p_{\Theta\vert X}(\theta\vert x)\\
=arg\max_{\theta}\frac{p_{X\vert \Theta}(x\vert \theta)p_{\Theta}(\theta)}{p_X(x)}$$

它们在形式上非常相似。上式分母和参数无关，所以只看分子。分子只和极大似然概率相差一个参数的先验分布。如果先验分布是常数，即均匀分布，
则两种方法得到的结果就完全一样了。所以我们也可以认为，**极大似然估计就是假设参数均匀分布的最大后验估计**。

因为极大似然估计也是观测X的函数，所以也是一个随机变量。同时因为X是真实参数 $\theta$ 的函数，所以估计量也是真实参数的函数 $\hat \Theta=h(\theta)$。
每个可能的参数会对应一个估计量（参数有多少可能的取值，就有多少个随机变量）。

下面分析一个好的估计应该有几点性质。注意这些性质需要对所有可能的真实参数成立，即参数所有可能取值对应的估计都要满足。

1. **无偏性 unbiased**，$E[\hat \Theta_n]=\theta$
2. **一致性 consistent**，$\hat \Theta_n$ 以概率收敛到 $\theta$
3. **平方误差小**，$E[(\hat \Theta-\theta)^2]=var(\hat \Theta-\theta)+(E[\hat \Theta-\theta])^2\\=var(\hat \Theta)+(bias)^2$

第二条表示当n很大时，估计应该以极大的概率等于真实参数。第三条表示如果平方误差很小，则估计的方差和bias都很小，
对应于精度（precision）和准确性（accuracy）都很高。

下面介绍另一种估计方法

### 求平均

这是我们最常用到的方法。适用于如下情况，所有观测Xi是独立同分布的，可以是任何分布，均值为 $\theta$ ，未知；方差为 $\sigma ^2$ ，已知。

$$X_i=\theta + W_i$$

Wi是0均值，方差为 $\sigma ^2$ 的噪声。则参数的估计为样本均值：

$$\hat \Theta_n=M_n=(X_1+...+X_n)/n$$

该估计满足上述几个性质：

$$E[\hat \Theta_n]=\theta\\
WLLN:\hat \Theta_n \rightarrow \theta\\
MSE:\sigma^2/n$$

如果噪声服从高斯分布，则样本均值和极大似然估计是一样的，否则可能不一样。但是因为样本均值容易计算，所以大部分情况下我们会使用样本均值。

### 置信区间

得到参数的估计后，我们如何描述真值可能出现的范围？因为估计值并不是真值，
而是真值可能出现的区域附近，我们怎么确定真值以多大的概率出现在某个区间内呢？

定义：一个 $1-\alpha$ 置信区间 $[\hat \Theta_n^-, \hat \Theta_n^+]$，其中两端都是随机变量，满足：

$$P(\hat \Theta_n^-\leq \theta \leq \hat \Theta_n^+)\geq 1-\alpha,\ \forall \theta$$

$\alpha$ 表示误差的大小，一般取0.05，0.01。95%置信区间表示，该区间有95%的概率覆盖真实参数。

对于样本均值来说，根据中心极限定理，样本均值可以归一化成标准正态分布，然后通过查表得到95%置信区间的范围是[-1.96,1.96]

$$P(\frac{\vert \hat \Theta_n-\theta\vert }{\sigma/\sqrt{n}}\leq 1.96)\approx 0.95\\
P(\hat \Theta-\frac{1.96\sigma}{\sqrt n}\leq \theta \leq \hat \Theta+\frac{1.96\sigma}{\sqrt n})\approx 0.95$$

上式不等式就表示了0.95置信区间的范围。我们也可以看到，n越大置信区间就越小。当然这个区间并不是精确的，但是当n很大的时候
这种近似是可以接受的。

如果我们不知道噪声的方差怎么办？有几种选择

1. 估计一个方差的上限，比如伯努利分布的方差小于等于 1/4
2. 如果我们知道模型，可以从样本中估计参数，进而计算方差
3. 如果不知道模型，则使用下面介绍的通用方法计算

根据弱大数定理，当n趋于无穷时下式以概率收敛于 $\sigma^2$

$$\frac{1}{n} \sum_{i=1}^n(X_i-\theta)^2$$

但是  是未知的，我们可以用估计量代替它，但是最终方差的无偏估计不再是除以n，而是n-1

$$S_n^2=\frac{1}{n-1} \sum_{i=1}^n(X_i-\hat \Theta_n)^2 \rightarrow \sigma^2\\
E[S_n^2]=\sigma^2$$

总结一下，首先根据样本估计均值，然后根据均值计算方差，最后用方差和均值得到置信区间。

### 例子

#### 线性回归

最后两节课是实际应用中的估计。首先是线性回归，给几组x，y，估计它们的线性关系。
一般会最小化平方误差，这里的概率基础是：假设噪声是高斯分布的，则极大似然估计的最终形式就是最小化平方误差。

所以我们经常使用的最小化平方误差，它背后有两个原理，一是中心极限定理，大量独立的因素影响下，噪声趋于高斯分布；
二是极大似然估计，化简后得到最小化平方误差。

最终的计算结果我们会得到x和y的比例系数为

$$\frac{cov(X,Y)}{var(X)}$$

这和我们之前推到的结果一样，即协方差的直观解释。同样如我们之前讨论过的，我们也可以求Y和 $X^2$ 或者更复杂形式函数的线性关系。
但是这里要注意，如果数据量很少，而函数很复杂则有可能**过拟合**。模型过于复杂，以至于过分拟合了带有噪声的数据，
这种模型在做预测的时候会得到非常不确定的结果。在机器学习中过拟合的讨论很多，如何选择模型和惩罚项是需要经验的。

有几个方法判断用线性模型做预测的效果好不好。

1. 参数的置信区间
2. 噪声的方差，方差大的预测不准
3.  $R^2$
4. 计算 $\frac{var(Y\vert X)}{var(Y)}$

最后一个式子这样理解：在没有观测的情况下，Y有方差这么大的不确定性，在有观测X后，因为有了额外的信息，
Y的不确定性也就是方差会变小。这个变小的程度就表示了X和Y的相关程度。根据这个公式可以估计Y的不确定性来自各个变量的成分有多少。

使用线性回归时也有几点注意：

1. 噪声分布不均匀，结果会受噪声大的数据的影响
2. 多重共线性 multicollinearity，X中有多个因素可以互相替代，数据小的扰动会导致结果变化很大
3. 线性关系不能确定X和Y谁是因，谁是果

#### 假设检验

假设检验中参数一般只有很少可能取值，比如 $H_0,H_1$，它们在观测X的空间上各自有概率分布，
一般我们会将X空间分成两个部分，当观测x落入H0对应的区域时我们接受H0，否则接受H1，或者说拒绝H0.

但是我们可能判断错误，比如真实参数是H0我们判断成H1，这个错误的概率可以记为 $\alpha$，
真实参数是H1我们判断成H0，这个错误的概率记为 $\beta$。当一个概率减小时另一个就会增加。

有两种思路分析假设检验：

1.贝叶斯最大后验概率，当满足下面的条件时，选择H1：

$$P(H_1\vert X=x)>P(H_0\vert X=x)\\
or\ \frac{P(X=x\vert H_1)P(H_1)}{P(X=x)}>\frac{P(X=x\vert H_0)P(H_0)}{P(X=x)}\\
or\ \frac{P(X=x\vert H_1)}{P(X=x\vert H_0)}>\frac{P(H_0)}{P(H_1)}$$

2. 非贝叶斯，满足下面的条件时，选择H1：

$$L(x)=\frac{P(X=x;H_1)}{P(X=x;H_0)}>\xi \tag 1$$

两种方法的判断阈值都可以根据要求选择，要求就是我们允许犯两种错误的概率是多少。

##### 二元假设检验

这里按照经典概率的方法来分析。一般我们会认为两个假设有一个是默认正确的，记为 $H_0$。
除非观测X落在拒绝域 R 中，我们才会拒绝 $H_0$ 而选择 $H_1$。
我们用似然比来判断是否拒绝H0，也就是上面的式1。

阈值 $\xi$ 的选择决定了出现上述两个错误的概率。一般我们会要求第一类错误概率小于5%，即 $\alpha=0.05$。
根据此要求选择 $\xi$ 作为判据使：

$$P(reject\ H_0;H_0)=\alpha$$

因为两类错误的概率不能同时减小，所以我们会希望固定 $\alpha$ 时， $\beta$ 能够尽可能小。
不加证明的说，使用似然比作为判据即可得到最优的结果。

##### 混合假设检验 composite hypotheses

该问题也有一个默认的假设 H0，不过另一个选择H1是除H0以外的所有可能性。
比如判断硬币正面向上的概率，原假设是 $H_0:p=0.5$，则备择假设 $H_1:p\neq 0.5$。

这类问题和上一类的区别在于备择假设不是一个分布，而是很多种分布构成。这时我们会分析观测X数据
服从原假设分布的可能性有多大，如果明显偏离了原假设模型，则拒绝原假设，否则接受原假设。

处理该问题分以下几步

1. 选择一个X的统计量S
2. 选择一个函数f，当 $f(S)>\xi$ 时拒绝H0
3. 确定犯第一类错误的概率，如0.05，进而确定 $\xi$

如果X没有落入拒绝域我们一般会说数据没有拒绝H0，而不是接受H0。这里有一点区别。
因为数据无法证明哪种假设才是正确的，比如精确的模型我们是得不到的，
只能在明显偏离假设模型的时候证明该模型错误。如果没有明显偏离，我们就没有理由拒绝该模型。

这和科学的证伪很像。我们提出一个科学理论，然后通过实验去证伪，如果不能证伪我们就没有理由拒绝该理论。
但我们不能说这个理论是绝对正确的，只是当前所有的数据不能推翻它，我们就可以用它。
在实验手段不够先进的时候牛顿力学是适用的，后来实验证明了牛顿力学有不适用的场景，而相对论更加适用。
如果有一天相对论被实验证伪，我们就再采用其他理论。

还有许多问题要解决：

* 更好的计算拒绝域的方法
* 如何根据数据计算 PDF
* 更快的计算方法以及实时处理数据
* ……

最后的总结，统计问题和之前学习的概率知识有很大的不同，可以认为是理论和实践的区别。
在使用概率知识解决实际统计问题的时候，基本上没有理想情况，和简单的模型。
需要大量的经验和trick，而这也造成了很多错误。

有一篇文章：why most published research findings are false。介绍了为什么大部分研究结果是错误的，
就是因为使用的统计方法有问题。我们应该先提出假设，然后做实验，而不是在得到数据之后，根据数据来提出假设，然后验证。
虽然犯错误的概率只有5%，但是当假设足够多时，就几乎必然会犯错误。

统计问题并不像概率问题有唯一确定的解，这是一个很麻烦的工作。
